{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIOkHdvQy0-Y"
   },
   "source": [
    "**В этом ноутбуке** пробуется модель на основе LSTM (так как у нас тут что-то похожее на временные ряды, то есть гипотеза, что LSTM хорошо здесь зайдет), обученная на предсказаниях леса. Ноутбук вдохновлен вот этим https://www.kaggle.com/khalildmk/simple-two-layer-bidirectional-lstm-with-pytorch - оттуда взято большинство кода, но была переделана работа с данными и размерности в самой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v4CuMDGu1Ebh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrLOU0aQ1f7k"
   },
   "source": [
    "### 1. Параметры и модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Dufs77m1OHA"
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "lr = 0.01\n",
    "n_folds = 5\n",
    "lstm_input_size = 12\n",
    "hidden_state_size = 30\n",
    "batch_size = 30\n",
    "num_sequence_layers = 2\n",
    "output_dim = 11\n",
    "num_time_steps = 4000\n",
    "rnn_type = 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzAMG9sD1efB"
   },
   "outputs": [],
   "source": [
    "class Bi_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=11, num_layers=2, rnn_type='LSTM'):\n",
    "        super(Bi_RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #Define the initial linear hidden layer\n",
    "        self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = eval('nn.' + rnn_type)(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #Forward pass through initial hidden layer\n",
    "        linear_input = self.init_linear(input)\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [batch_size, input_size ,hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both\n",
    "        # have shape (batch_size, num_layers, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(linear_input)\n",
    "\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hebEU-O2XoA"
   },
   "source": [
    "### 2. Даталоадеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rsPKzeJn1shk"
   },
   "outputs": [],
   "source": [
    "class ION_Dataset_Sequential(Dataset):\n",
    "    def __init__(self, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        y = self.output[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "class ION_Dataset_Sequential_test(Dataset):\n",
    "    def __init__(self, input):\n",
    "        self.input = input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GaJGDOt3N8q"
   },
   "source": [
    "### 3. Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZggP3v6o3TEa",
    "outputId": "806be69c-783e-4f81-a342-103820414e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ciBbNdQ08rBE"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/content/gdrive/My Drive/data/train_clean.csv')\n",
    "test_df = pd.read_csv('/content/gdrive/My Drive/data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bySuAzog8sYC"
   },
   "outputs": [],
   "source": [
    "train_probs = np.load('/content/gdrive/My Drive/data/Y_train_proba.npy')\n",
    "test_probs = np.load('/content/gdrive/My Drive/data/Y_test_proba.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XSevUjFa82zP"
   },
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, pd.DataFrame(train_probs)], axis=1)\n",
    "test_df = pd.concat([test_df, pd.DataFrame(test_probs)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2VFBTTS-Zp7"
   },
   "outputs": [],
   "source": [
    "train_df.columns = train_df.columns.astype(str)\n",
    "test_df.columns = test_df.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wb9MurSk3M7r"
   },
   "outputs": [],
   "source": [
    "X = train_df[['signal', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']].values.reshape(-1, num_time_steps, 12)\n",
    "y = pd.get_dummies(train_df['open_channels']).values.reshape(-1, num_time_steps, output_dim)\n",
    "test_input = test_df[['signal', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']].values.reshape(-1, num_time_steps, 12)\n",
    "train_input_mean = X.mean()\n",
    "train_input_sigma = X.std()\n",
    "test_input = (test_input-train_input_mean)/train_input_sigma\n",
    "test_preds = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n",
    "test = ION_Dataset_Sequential_test(test_input)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGdAFbZlITlW"
   },
   "source": [
    "### 4. Обучение с разбиением на фолды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OrXtoc2J6oRL",
    "outputId": "2003ff15-e84d-4bee-d6aa-2da68a74c253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting fold 0\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 12)\n",
      "###### Loading\n",
      "Epoch 1/100 \t loss=0.2928 \t train_f1=0.1054 \t val_loss=0.2055 \t val_f1=0.2158 \t time=41.63s\n",
      "Epoch 11/100 \t loss=0.0204 \t train_f1=0.8489 \t val_loss=0.0238 \t val_f1=0.8492 \t time=42.01s\n",
      "Epoch 21/100 \t loss=0.0165 \t train_f1=0.9382 \t val_loss=0.0193 \t val_f1=0.9373 \t time=41.90s\n",
      "Epoch 31/100 \t loss=0.0164 \t train_f1=0.9377 \t val_loss=0.0188 \t val_f1=0.9372 \t time=41.97s\n",
      "Epoch 41/100 \t loss=0.0156 \t train_f1=0.9384 \t val_loss=0.0185 \t val_f1=0.9372 \t time=42.19s\n",
      "Epoch 51/100 \t loss=0.0157 \t train_f1=0.9382 \t val_loss=0.0185 \t val_f1=0.9374 \t time=41.84s\n",
      "Epoch 61/100 \t loss=0.0162 \t train_f1=0.9379 \t val_loss=0.0190 \t val_f1=0.9371 \t time=42.07s\n",
      "Epoch 71/100 \t loss=0.0157 \t train_f1=0.9383 \t val_loss=0.0186 \t val_f1=0.9375 \t time=41.90s\n",
      "Epoch 81/100 \t loss=0.0156 \t train_f1=0.9383 \t val_loss=0.0184 \t val_f1=0.9376 \t time=42.10s\n",
      "Epoch 91/100 \t loss=0.0152 \t train_f1=0.9383 \t val_loss=0.0184 \t val_f1=0.9375 \t time=41.94s\n",
      "BEST VALIDATION SCORE (F1):  0.9377366523157712\n",
      "starting fold 1\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 12)\n",
      "###### Loading\n",
      "Epoch 1/100 \t loss=0.2628 \t train_f1=0.1831 \t val_loss=0.1796 \t val_f1=0.2677 \t time=41.80s\n",
      "Epoch 11/100 \t loss=0.0209 \t train_f1=0.8492 \t val_loss=0.0259 \t val_f1=0.8491 \t time=41.80s\n",
      "Epoch 21/100 \t loss=0.0168 \t train_f1=0.9378 \t val_loss=0.0208 \t val_f1=0.9368 \t time=41.91s\n",
      "Epoch 31/100 \t loss=0.0156 \t train_f1=0.9381 \t val_loss=0.0198 \t val_f1=0.9371 \t time=41.90s\n",
      "Epoch 41/100 \t loss=0.0159 \t train_f1=0.9379 \t val_loss=0.0197 \t val_f1=0.9371 \t time=41.70s\n",
      "Epoch 51/100 \t loss=0.0154 \t train_f1=0.9382 \t val_loss=0.0199 \t val_f1=0.9359 \t time=41.60s\n",
      "Epoch 61/100 \t loss=0.0151 \t train_f1=0.9383 \t val_loss=0.0196 \t val_f1=0.9368 \t time=41.82s\n",
      "Epoch 71/100 \t loss=0.0165 \t train_f1=0.9370 \t val_loss=0.0200 \t val_f1=0.9369 \t time=41.43s\n",
      "Epoch 81/100 \t loss=0.0152 \t train_f1=0.9381 \t val_loss=0.0193 \t val_f1=0.9372 \t time=41.68s\n",
      "Epoch 91/100 \t loss=0.0153 \t train_f1=0.9382 \t val_loss=0.0194 \t val_f1=0.9371 \t time=41.30s\n",
      "BEST VALIDATION SCORE (F1):  0.9376231785584731\n",
      "starting fold 2\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 12)\n",
      "###### Loading\n",
      "Epoch 1/100 \t loss=0.2652 \t train_f1=0.1949 \t val_loss=0.1760 \t val_f1=0.2983 \t time=41.88s\n",
      "Epoch 11/100 \t loss=0.0225 \t train_f1=0.8494 \t val_loss=0.0252 \t val_f1=0.8475 \t time=41.57s\n",
      "Epoch 21/100 \t loss=0.0170 \t train_f1=0.9378 \t val_loss=0.0192 \t val_f1=0.9370 \t time=40.97s\n",
      "Epoch 31/100 \t loss=0.0167 \t train_f1=0.9367 \t val_loss=0.0193 \t val_f1=0.9366 \t time=40.41s\n",
      "Epoch 41/100 \t loss=0.0162 \t train_f1=0.9379 \t val_loss=0.0182 \t val_f1=0.9375 \t time=40.79s\n",
      "Epoch 51/100 \t loss=0.0156 \t train_f1=0.9381 \t val_loss=0.0181 \t val_f1=0.9375 \t time=40.29s\n",
      "Epoch 61/100 \t loss=0.0157 \t train_f1=0.9381 \t val_loss=0.0179 \t val_f1=0.9377 \t time=40.46s\n",
      "Epoch 71/100 \t loss=0.0156 \t train_f1=0.9379 \t val_loss=0.0178 \t val_f1=0.9377 \t time=40.37s\n",
      "Epoch 81/100 \t loss=0.0158 \t train_f1=0.9374 \t val_loss=0.0182 \t val_f1=0.9369 \t time=40.44s\n",
      "Epoch 91/100 \t loss=0.0158 \t train_f1=0.9380 \t val_loss=0.0185 \t val_f1=0.9368 \t time=40.86s\n",
      "BEST VALIDATION SCORE (F1):  0.9380991640612855\n",
      "starting fold 3\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 12)\n",
      "###### Loading\n",
      "Epoch 1/100 \t loss=0.2821 \t train_f1=0.1583 \t val_loss=0.1878 \t val_f1=0.2488 \t time=40.29s\n",
      "Epoch 11/100 \t loss=0.0225 \t train_f1=0.8489 \t val_loss=0.0228 \t val_f1=0.8505 \t time=40.34s\n",
      "Epoch 21/100 \t loss=0.0174 \t train_f1=0.9378 \t val_loss=0.0177 \t val_f1=0.9395 \t time=40.54s\n",
      "Epoch 31/100 \t loss=0.0162 \t train_f1=0.9375 \t val_loss=0.0172 \t val_f1=0.9388 \t time=40.92s\n",
      "Epoch 41/100 \t loss=0.0161 \t train_f1=0.9375 \t val_loss=0.0175 \t val_f1=0.9390 \t time=40.51s\n",
      "Epoch 51/100 \t loss=0.0159 \t train_f1=0.9378 \t val_loss=0.0166 \t val_f1=0.9396 \t time=40.60s\n",
      "Epoch 61/100 \t loss=0.0160 \t train_f1=0.9378 \t val_loss=0.0165 \t val_f1=0.9396 \t time=40.51s\n",
      "Epoch 71/100 \t loss=0.0160 \t train_f1=0.9375 \t val_loss=0.0164 \t val_f1=0.9394 \t time=40.49s\n",
      "Epoch 81/100 \t loss=0.0157 \t train_f1=0.9378 \t val_loss=0.0165 \t val_f1=0.9396 \t time=40.96s\n",
      "Epoch 91/100 \t loss=0.0159 \t train_f1=0.9379 \t val_loss=0.0165 \t val_f1=0.9398 \t time=40.47s\n",
      "BEST VALIDATION SCORE (F1):  0.93980716248632\n",
      "starting fold 4\n",
      "###### splitting and reshaping the data\n",
      "(1000, 4000, 12)\n",
      "###### Loading\n",
      "Epoch 1/100 \t loss=0.2710 \t train_f1=0.1693 \t val_loss=0.1904 \t val_f1=0.2708 \t time=40.36s\n",
      "Epoch 11/100 \t loss=0.0208 \t train_f1=0.8489 \t val_loss=0.0254 \t val_f1=0.8496 \t time=40.46s\n",
      "Epoch 21/100 \t loss=0.0159 \t train_f1=0.9380 \t val_loss=0.0203 \t val_f1=0.9375 \t time=41.19s\n",
      "Epoch 31/100 \t loss=0.0155 \t train_f1=0.9381 \t val_loss=0.0199 \t val_f1=0.9379 \t time=40.58s\n",
      "Epoch 41/100 \t loss=0.0153 \t train_f1=0.9383 \t val_loss=0.0196 \t val_f1=0.9378 \t time=40.38s\n",
      "Epoch 51/100 \t loss=0.0154 \t train_f1=0.9379 \t val_loss=0.0194 \t val_f1=0.9380 \t time=40.27s\n",
      "Epoch 61/100 \t loss=0.0152 \t train_f1=0.9383 \t val_loss=0.0195 \t val_f1=0.9382 \t time=40.18s\n",
      "Epoch 71/100 \t loss=0.0151 \t train_f1=0.9379 \t val_loss=0.0199 \t val_f1=0.9377 \t time=40.22s\n",
      "Epoch 81/100 \t loss=0.0153 \t train_f1=0.9379 \t val_loss=0.0194 \t val_f1=0.9382 \t time=40.19s\n",
      "Epoch 91/100 \t loss=0.0151 \t train_f1=0.9378 \t val_loss=0.0196 \t val_f1=0.9375 \t time=40.18s\n",
      "BEST VALIDATION SCORE (F1):  0.9384948498276348\n",
      "Final Score  0.938352201449897\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "local_val_score = 0\n",
    "models = {}\n",
    "\n",
    "k=0 #initialize fold number\n",
    "for tr_idx, val_idx in kfold.split(X, y):\n",
    "    test_p = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n",
    "\n",
    "    print('starting fold', k)\n",
    "    k += 1\n",
    "\n",
    "    print(6*'#', 'splitting and reshaping the data')\n",
    "    train_input = X[tr_idx]\n",
    "    print(train_input.shape)\n",
    "    train_target = y[tr_idx]\n",
    "    val_input = X[val_idx]\n",
    "    val_target = y[val_idx]\n",
    "    train_input_mean = train_input.mean()\n",
    "    train_input_sigma = train_input.std()\n",
    "    val_input = (val_input-train_input_mean)/train_input_sigma\n",
    "    train_input = (train_input-train_input_mean)/train_input_sigma\n",
    "\n",
    "    print(6*'#', 'Loading')\n",
    "    train = ION_Dataset_Sequential(train_input, train_target)\n",
    "    valid = ION_Dataset_Sequential(val_input, val_target)\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    #Build tensor data for torch\n",
    "    train_preds = np.zeros((int(train_input.shape[0] * train_input.shape[1])))\n",
    "    val_preds = np.zeros((int(val_input.shape[0] * val_input.shape[1])))\n",
    "    best_val_preds = np.zeros((int(val_input.shape[0] * val_input.shape[1])))\n",
    "    train_targets = np.zeros((int(train_input.shape[0] * train_input.shape[1])))\n",
    "    avg_losses_f = []\n",
    "    avg_val_losses_f = []\n",
    "\n",
    "    #Define loss function\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    #Build model, initialize weights and define optimizer\n",
    "    model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # Using Adam optimizer\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=150, factor=0.1, min_lr=1e-8)  # Using ReduceLROnPlateau schedule\n",
    "    temp_val_loss = 9999999999\n",
    "    reached_val_score = 0\n",
    "\n",
    "    #Iterate through epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        #Train\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "            y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch.cuda())\n",
    "            loss = loss_fn(y_pred.cpu(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "            train_preds[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = pred.reshape((-1))\n",
    "            train_targets[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = y_batch.detach().cpu().numpy().argmax(axis=2).reshape((-1))\n",
    "            del y_pred, loss, x_batch, y_batch, pred\n",
    "\n",
    "        #Evaluate\n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "            y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "            y_pred = model(x_batch.cuda()).detach()\n",
    "            avg_val_loss += loss_fn(y_pred.cpu(), y_batch).item() / len(valid_loader)\n",
    "            pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "            val_preds[i * batch_size * val_input.shape[1]:(i + 1) * batch_size * val_input.shape[1]] = pred.reshape((-1))\n",
    "            del y_pred, x_batch, y_batch, pred\n",
    "        if avg_val_loss < temp_val_loss:\n",
    "            temp_val_loss = avg_val_loss\n",
    "\n",
    "        #Calculate F1-score\n",
    "        train_score = f1_score(train_targets, train_preds, average='macro')\n",
    "        val_score = f1_score(val_target.argmax(axis=2).reshape((-1)), val_preds, average='macro')\n",
    "\n",
    "        #Print output of epoch\n",
    "        elapsed_time = time.time() - start_time\n",
    "        scheduler.step(avg_val_loss)\n",
    "        if epoch%10 == 0:\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t train_f1={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, avg_loss, train_score, avg_val_loss, val_score, elapsed_time))\n",
    "\n",
    "        if val_score > reached_val_score:\n",
    "            reached_val_score = val_score\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_val_preds = copy.deepcopy(val_preds)\n",
    "\n",
    "    #Calculate F1-score of the fold\n",
    "    val_score_fold = f1_score(val_target.argmax(axis=2).reshape((-1)), best_val_preds, average='macro')\n",
    "\n",
    "    #Save the fold's model in a dictionary\n",
    "    models[k] = best_model\n",
    "\n",
    "    #Print F1-score of the fold\n",
    "    print(\"BEST VALIDATION SCORE (F1): \", val_score_fold)\n",
    "    local_val_score += (1/n_folds) * val_score_fold\n",
    "\n",
    "#Print final average k-fold CV F1-score\n",
    "print(\"Final Score \", local_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3362pskUKbz"
   },
   "source": [
    "### 5. Предсказание - усреднение предсказаний моделей, обученных на разных фолдах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2paZ8RVQURjO"
   },
   "outputs": [],
   "source": [
    "for k in range(n_folds):\n",
    "    test_p = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n",
    "    k += 1\n",
    "\n",
    "    #Import model of fold k\n",
    "    model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(models[k])\n",
    "\n",
    "    #Make predictions on test data\n",
    "    model.eval()\n",
    "    for i, x_batch in enumerate(test_loader):\n",
    "        x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "        y_pred = model(x_batch.cuda()).detach()\n",
    "        pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "        test_p[i * batch_size * test_input.shape[1]:(i + 1) * batch_size * test_input.shape[1]] = pred.reshape((-1))\n",
    "        del y_pred, x_batch, pred\n",
    "    test_preds += (1/n_folds) * test_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6v3q3r0nY4SW"
   },
   "source": [
    "### 6. Сохраняем ответы в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vt-_Vr-FY_Zj"
   },
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"/content/gdrive/My Drive/data/sample_submission.csv\", dtype = {'time': str})\n",
    "df_sub.open_channels = np.array(test_preds, np.int)\n",
    "df_sub.to_csv(\"/content/gdrive/My Drive/data/submission_bilstm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v575lRAUUefH"
   },
   "source": [
    "**Результат:** 0.939 на public lb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_stacking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
